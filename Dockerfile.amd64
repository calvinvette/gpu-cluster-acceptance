# SPDX-License-Identifier: AGPL-3.0-only
# Copyright (c) 2025 Calvin Vette
# Dockerfile.amd64: GPU System Acceptance Testing 
#   For AMD64 (Intel or AMD) motherboards with 
#   either 1 or 8 A100, H100, H200, or B100, or B200 graphics cards
#   with NVLink on 2 or more GPU boxes
#   and Infiniband on multiple 8-GPU boxes networks

# ---- builder ----
# FROM nvcr.io/nvidia/cuda:12.4.1-devel-ubuntu22.04 AS builder
FROM docker.io/nvidia/cuda:12.4.1-devel-ubuntu22.04 AS builder
# ARGS: (TODO - Set here and parameterize below)
    # CUDA_VERSION=12.4.1
    # CUDA_MAJOR_VERSION=12
    # CUDA_MINOR_VERSION=4
    # CUDA_PATCH_VERSION=1
    # CUDA_CRAMMED_VERSION=124
    # HPCX_VERSION=2.23
    # UBUNTU_VERSION=22.04 # NVIDIA still has issues on 24.04
    # PYTHON_VERSION=3.12
    # TORCH_VERSION=2.8.0
    # SMS="87"            
    # CUDA_ARCH_BIN="87"
    # CUDA_HOME="/usr/local/cuda"
    # FLASH_ATTN_VERION=v2.8.2

RUN apt-get update && DEBIAN_FRONTEND=noninteractive apt-get install -y \
    git build-essential cmake libopenmpi-dev openmpi-bin && \
    rm -rf /var/lib/apt/lists/*

# nvbandwidth
RUN git clone --depth 1 https://github.com/NVIDIA/nvbandwidth /opt/nvbandwidth
WORKDIR /opt/nvbandwidth
# debian_install failed - doing these steps manually
# RUN . ./debian_install.sh
RUN apt-get -y update
RUN apt-get install -y software-properties-common apt-transport-https curl wget
RUN apt-get -y update
RUN add-apt-repository universe
RUN apt-get -y update
RUN apt-get install -y libboost-all-dev libboost-program-options-dev
RUN mkdir -p /opt/nvbandwidth/build
WORKDIR /opt/nvbandwidth/build
RUN cmake ..
RUN make -j ${nproc} -C /opt/nvbandwidth/build

# HPCX
WORKDIR /tmp
# RUN curl -LO https://content.mellanox.com/hpc/hpc-x/v2.23/hpcx-v2.23-gcc-inbox-ubuntu22.04-cuda12-x86_64.tbz
RUN curl -LO 'https://content.mellanox.com/hpc/hpc-x/v2.23/hpcx-v2.23-gcc-doca_ofed-ubuntu22.04-cuda12-x86_64.tbz'
# RUN wget -O 'https://content.mellanox.com/hpc/hpc-x/v2.23/hpcx-v2.23-gcc-doca_ofed-ubuntu22.04-cuda12-x86_64.tbz'
RUN tar -C /opt -xvpf hpcx-v2.23-gcc-doca_ofed-ubuntu22.04-cuda12-x86_64.tbz
RUN rm /tmp/hpcx-v2.23-gcc-doca_ofed-ubuntu22.04-cuda12-x86_64.tbz
RUN ln -s /opt/hpcx-v2.23-gcc-doca_ofed-ubuntu22.04-cuda12-x86_64 /opt/hpcx
# Setting the environment via script failed; doing manual ENV commands instead
# RUN . /opt/hpcx/hpcx-init.sh
ENV HPCX_DIR="/opt/hpcx"
ENV HPCX_UCX_DIR="/opt/hpcx/ucx"
ENV HPCX_UCC_DIR="/opt/hpcx/ucc"
ENV HPCX_SHARP_DIR="/opt/hpcx/sharp"
ENV HPCX_HCOLL_DIR="/opt/hpcx/hcoll"
ENV HPCX_MPI_DIR="/opt/hpcx/ompi"
ENV HPCX_OSHMEM_DIR="/opt/hpcx/ompi"
ENV HPCX_MPI_TESTS_DIR="/opt/hpcx/ompi/tests"
ENV HPCX_OSU_DIR="/opt/hpcx/ompi/tests/osu-micro-benchmarks"
ENV HPCX_OSU_CUDA_DIR="/opt/hpcx/ompi/tests/osu-micro-benchmarks-cuda"
ENV HPCX_CLUSTERKIT_DIR="/opt/hpcx/clusterkit"
ENV HPCX_NCCLNET_PLUGIN_DIR="/opt/hpcx/ncclnet_plugin"
ENV HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR="/opt/hpcx/nccl_rdma_sharp_plugin"
ENV OMPI_HOME="/opt/hpcx/ompi"
ENV MPI_HOME="/opt/hpcx/ompi"
ENV OSHMEM_HOME="/opt/hpcx/ompi"
ENV SHMEM_HOME="/opt/hpcx/ompi"
ENV PATH="/usr/local/cuda-12/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin:/opt/nvbandwidth:/opt/hpcx/:/opt/hpcx/ucc/bin:/opt/hpcx/ucx/bin:/opt/hpcx/sharp/bin:/opt/hpcx/ompi/bin:/opt/hpcx/clusterkit/bin:/opt/hpcx/hcoll/bin:${PATH}"
ENV LD_LIBRARY_PATH="/usr/local/lib:/usr/local/cuda-12/lib64:${LD_LIBRARY_PATH}"

# nccl-tests
RUN git clone --depth 1 https://github.com/NVIDIA/nccl-tests /opt/nccl-tests
RUN make -j ${nproc} -C /opt/nccl-tests MPI=1

# Upgrade pip and install uv
RUN apt-get install -y python3-pip
RUN pip3 install --no-cache-dir --upgrade pip
RUN pip3 install packaging uv

# Compile Flash Attention
# TODO - optimize this build (see NGE/torch-cuda/build.sh)
# RUN uv pip install --system flash-attn==2.8.2 --no-build-isolation
# First build flash_attention
RUN mkdir -p /workspace
WORKDIR /workspace
# TODO - Parameterize the GPU Arch
# Build for Ampere and above; Consider flash-attn3 for Blackwell
ENV SMS="87"            
ENV CUDA_ARCH_BIN="87"
ENV CUDA_HOME="/usr/local/cuda"
# TODO - Parameterize the version and file name to make upgrades easier
RUN git clone --branch v2.8.2 --depth 1 https://github.com/Dao-AILab/flash-attention.git
# First install torch and friends
# PyTorch + CUDA 12.4 wheel (smaller than NGC PyTorch image)
RUN uv pip install --system --no-cache-dir --index-url https://download.pytorch.org/whl/cu124 \
    torch \
    torchvision \
    torchaudio

RUN cd flash-attention && MAX_JOBS=8 python3 setup.py build develop bdist_wheel
# Wheel should be here:
RUN ls -la flash-attention/dist/

# Build the DCGM Exporter
# Note that DCGM Exporter is already installed by the NVIDIA GPU Operator in k8s
# However we'll use it here for the acceptance testing independently
WORKDIR /workspace
RUN git clone --depth 1 https://github.com/NVIDIA/dcgm-exporter.git
WORKDIR /workspace/dcgm-exporter
RUN apt-get install -y golang
RUN go mod tidy
RUN make -j ${nproc} binary && make install



# ---- runtime ----
# FROM nvcr.io/nvidia/cuda:12.4.1-runtime-ubuntu22.04
FROM docker.io/nvidia/cuda:12.4.1-runtime-ubuntu22.04

LABEL org.opencontainers.image.licenses="AGPL-3.0-only"

# minimal runtime deps
RUN apt-get update && DEBIAN_FRONTEND=noninteractive apt-get install -y \
    python3 python3-pip python3-venv \
    rdma-core libibverbs1 libopenmpi3 openmpi-bin \
    pciutils iproute2 iputils-ping jq fio && \
    rm -rf /var/lib/apt/lists/*

# (Optional) DCGM tools inside container if you want to run dcgmi from here
# TODO - Fix this - Apparently we can't have nice things... 
# RUN apt-get install -y dcgm dcgm-exporter && rm -rf /var/lib/apt/lists/*

# Python deps (install only what you need)
COPY pyproject.toml /app/pyproject.toml
WORKDIR /app
RUN pip3 install --no-cache-dir --upgrade pip
RUN pip3 install packaging uvicorn uv


# Copy binaries from builder
COPY --from=builder /opt/nvbandwidth /opt/nvbandwidth
COPY --from=builder /opt/nccl-tests /opt/nccl-tests
COPY --from=builder /opt/hcx /opt/hcx
COPY --from=builder /opt/hpcx-v2.23-gcc-doca_ofed-ubuntu22.04-cuda12-x86_64 /opt/hpcx-v2.23-gcc-doca_ofed-ubuntu22.04-cuda12-x86_64
RUN ln -s /opt/hpcx-v2.23-gcc-doca_ofed-ubuntu22.04-cuda12-x86_64 /opt/hpcx

# Copy the flash_attn we built in the builder and install it
# TODO - Parameterize the file name to make upgrades easier
COPY --from=builder /workspace/flash-attention/dist/flash_attn-2.8.2-cp310-cp310-linux_amd64.whl /tmp/flash_attn-2.8.2-cp310-cp310-linux_amd64.whl
RUN uv pip install --system /tmp/flash_attn-2.8.2-cp310-cp310-linux_amd64.whl

# PyTorch + CUDA 12.4 wheel (smaller than NGC PyTorch image)
RUN uv pip install --system --no-cache-dir --index-url https://download.pytorch.org/whl/cu124 \
    torch \
    torchvision \
    torchaudio

# Now add the model training app.
RUN uv pip install --system -e /app

# Non-root user & workspace
RUN useradd -ms /bin/bash trainer && mkdir -p /workspace && chown -R trainer:trainer /workspace /opt
USER trainer
WORKDIR /workspace

COPY --chown=trainer:trainer scripts/ ./scripts/
COPY --chown=trainer:trainer training/ ./training/
COPY --chown=trainer:trainer configs/ ./configs/

ENV PATH="/opt/nvbandwidth:/opt/nccl-tests:/opt/hpcx/:/opt/hpcx/ucc/bin:/opt/hpcx/ucx/bin:/opt/hpcx/sharp/bin:/opt/hpcx/ompi/bin:/opt/hpcx/clusterkit/bin:/opt/hpcx/hcoll/bin:${PATH}"
