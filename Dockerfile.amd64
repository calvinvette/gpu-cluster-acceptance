# SPDX-License-Identifier: AGPL-3.0-only
# Copyright (c) 2025 Calvin Vette
# Dockerfile.amd64: GPU System Acceptance Testing 
#   For AMD64 (Intel or AMD) motherboards with 
#   either 1 or 8 A100, H100, H200, or B100, or B200 graphics cards
#   with NVLink on 2 or more GPU boxes
#   and Infiniband on multiple 8-GPU boxes networks

# ---- builder ----
# FROM nvcr.io/nvidia/cuda:12.4.1-devel-ubuntu22.04 AS builder
FROM docker.io/nvidia/cuda:12.4.1-devel-ubuntu22.04 AS builder
# ARGS: (TODO - Set here and parameterize below)
    # CUDA_VERSION=12.4.1
    # CUDA_MAJOR_VERSION=12
    # CUDA_MINOR_VERSION=4
    # CUDA_PATCH_VERSION=1
    # CUDA_CRAMMED_VERSION=124
    # HPCX_VERSION=2.23
    # UBUNTU_VERSION=22.04 # NVIDIA still has issues on 24.04
    # PYTHON_VERSION=3.12
    # TORCH_VERSION=2.8.0
    # SMS="87"            
    # CUDA_ARCH_BIN="87"
    # CUDA_HOME="/usr/local/cuda"
    # FLASH_ATTN_VERION=v2.8.2
    # DCGM_EXPORTER=4.3.1-4.4.0
    # DCGM

ENV SMS="87"            
ENV CUDA_ARCH_BIN=${SMS}

RUN apt-get update && DEBIAN_FRONTEND=noninteractive apt-get install -y \
    git build-essential gcc make cmake ninja-build libopenmpi-dev openmpi-bin && \
    rm -rf /var/lib/apt/lists/*

# nvbandwidth
RUN git clone --depth 1 https://github.com/NVIDIA/nvbandwidth /opt/nvbandwidth
WORKDIR /opt/nvbandwidth
# debian_install failed - doing these steps manually
# RUN . ./debian_install.sh
RUN apt-get -y update
RUN apt-get install -y software-properties-common apt-transport-https curl wget
RUN apt-get -y update
RUN add-apt-repository universe
RUN apt-get -y update
RUN apt-get install -y libboost-all-dev libboost-program-options-dev
RUN mkdir -p /opt/nvbandwidth/build
WORKDIR /opt/nvbandwidth/build
RUN cmake ..
RUN make -j$(nproc) -C /opt/nvbandwidth/build

# HPCX
WORKDIR /tmp
# RUN curl -LO https://content.mellanox.com/hpc/hpc-x/v2.23/hpcx-v2.23-gcc-inbox-ubuntu22.04-cuda12-x86_64.tbz
RUN curl -LO 'https://content.mellanox.com/hpc/hpc-x/v2.23/hpcx-v2.23-gcc-doca_ofed-ubuntu22.04-cuda12-x86_64.tbz'
# RUN wget -O 'https://content.mellanox.com/hpc/hpc-x/v2.23/hpcx-v2.23-gcc-doca_ofed-ubuntu22.04-cuda12-x86_64.tbz'
RUN tar -C /opt -xvpf hpcx-v2.23-gcc-doca_ofed-ubuntu22.04-cuda12-x86_64.tbz
RUN ln -s /opt/hpcx-v2.23-gcc-doca_ofed-ubuntu22.04-cuda12-x86_64 /opt/hpcx
# Setting the environment via script failed; doing manual ENV commands instead
# RUN . /opt/hpcx/hpcx-init.sh
ENV HPCX_DIR="/opt/hpcx"
ENV HPCX_UCX_DIR="/opt/hpcx/ucx"
ENV HPCX_UCC_DIR="/opt/hpcx/ucc"
ENV HPCX_SHARP_DIR="/opt/hpcx/sharp"
ENV HPCX_HCOLL_DIR="/opt/hpcx/hcoll"
ENV HPCX_MPI_DIR="/opt/hpcx/ompi"
ENV HPCX_OSHMEM_DIR="/opt/hpcx/ompi"
ENV HPCX_MPI_TESTS_DIR="/opt/hpcx/ompi/tests"
ENV HPCX_OSU_DIR="/opt/hpcx/ompi/tests/osu-micro-benchmarks"
ENV HPCX_OSU_CUDA_DIR="/opt/hpcx/ompi/tests/osu-micro-benchmarks-cuda"
ENV HPCX_CLUSTERKIT_DIR="/opt/hpcx/clusterkit"
ENV HPCX_NCCLNET_PLUGIN_DIR="/opt/hpcx/ncclnet_plugin"
ENV HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR="/opt/hpcx/nccl_rdma_sharp_plugin"
ENV OMPI_HOME="/opt/hpcx/ompi"
ENV MPI_HOME="/opt/hpcx/ompi"
ENV OSHMEM_HOME="/opt/hpcx/ompi"
ENV SHMEM_HOME="/opt/hpcx/ompi"
ENV PATH="/usr/local/cuda-12/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin:/opt/nvbandwidth:/opt/hpcx/:/opt/hpcx/ucc/bin:/opt/hpcx/ucx/bin:/opt/hpcx/sharp/bin:/opt/hpcx/ompi/bin:/opt/hpcx/clusterkit/bin:/opt/hpcx/hcoll/bin:${PATH}"
ENV LD_LIBRARY_PATH="/usr/local/lib:/usr/local/cuda-12/lib64:${LD_LIBRARY_PATH}"

# nccl-tests
RUN git clone --depth 1 https://github.com/NVIDIA/nccl-tests /opt/nccl-tests
RUN make -j$(nproc) -C /opt/nccl-tests MPI=1

# Upgrade pip and install uv
RUN apt-get install -y python3-pip
RUN pip3 install --no-cache-dir --upgrade pip
RUN pip3 install packaging uv

# Flash Attention should be compiled with optimizations and dumped in ./out
# It can be a real beast to compile, so it should be done externally (as part of a build pipeline)
# See dockerfiles/Dockerfile.flash-attention.${arch} and 2_flash_attention_build.sh
# Alternatively, download a pre-compiled copy and dump it there
# TODO - make this work better with build arguments

# Build the DCGM Exporter
# Note that DCGM Exporter is already installed by the NVIDIA GPU Operator in k8s
# However we'll use it here for the acceptance testing independently
# First we need to update golang from 1.18 to something more recent
WORKDIR /workspace

# First, remove the ancient version of go (1.18) that comes installed and replace it with modern go.
RUN apt-get remove -y golang-go
RUN wget https://go.dev/dl/go1.25.0.linux-amd64.tar.gz
RUN tar -C /usr/local/ -xvpf go1.25.0.linux-amd64.tar.gz
ENV GO_HOME=/usr/local/go
ENV PATH=${GO_HOME}/bin:$PATH

RUN git clone --depth 1 https://github.com/NVIDIA/DCGM.git /workspace/dcgm
WORKDIR /workspace/dcgm
# RUN ./build.sh
# RUN make -j$(nproc) binary && make install

RUN git clone https://github.com/NVIDIA/dcgm-exporter.git /workspace/dcgm-exporter
WORKDIR /workspace/dcgm-exporter
RUN git checkout 4.3.1-4.4.0
RUN make -j$(nproc) binary && make install

RUN git clone --depth 1 https://github.com/wilicc/gpu-burn.git /workspace/gpu-burn
WORKDIR /workspace/gpu-burn
RUN make -j$(nproc) COMPUTE=${SMS}


# ---- runtime ----
# TODO - revert this back to runtime and fill in the minimal parts from develop that are necessary
# FROM nvcr.io/nvidia/cuda:12.4.1-runtime-ubuntu22.04
FROM docker.io/nvidia/cuda:12.4.1-devel-ubuntu22.04

LABEL org.opencontainers.image.licenses="AGPL-3.0-only"

# minimal runtime deps
RUN apt-get update && DEBIAN_FRONTEND=noninteractive apt-get install -y \
    python3 python3-pip python3-venv \
    rdma-core libibverbs1 libopenmpi3 openmpi-bin \
    pciutils iproute2 iputils-ping jq fio curl wget 
    # Disable this until we've dialed it all in
    # && \
    # rm -rf /var/lib/apt/lists/*

# Python deps (install only what you need)
COPY pyproject.toml /app/pyproject.toml
WORKDIR /app
RUN pip3 install --no-cache-dir --upgrade pip
RUN pip3 install packaging uvicorn uv

# Copy binaries from builder
COPY --from=builder /opt/nvbandwidth /opt/nvbandwidth
COPY --from=builder /opt/nccl-tests /opt/nccl-tests
COPY --from=builder /opt/hpcx-v2.23-gcc-doca_ofed-ubuntu22.04-cuda12-x86_64 /opt/hpcx-v2.23-gcc-doca_ofed-ubuntu22.04-cuda12-x86_64
RUN ln -s /opt/hpcx-v2.23-gcc-doca_ofed-ubuntu22.04-cuda12-x86_64 /opt/hpcx

# Copy the flash_attn we built in the previous builder in the pipeline and install it
# TODO - Parameterize the file name to make upgrades easier
# Wheel should be here (see instructions in builder above, or Dockerfile/scripts in dockerfiles/):
# TODO - make this work better with build arguments
# Comment these two lines out for GitHub actions
COPY out/flash_attn-2.8.2-cp310-cp310-linux_x86_64.whl /tmp/flash_attn-2.8.2-cp310-cp310-linux_x86_64.whl
RUN uv pip install --system /tmp/flash_attn-2.8.2-cp310-cp310-linux_x86_64.whl
# Uncomment out this line for GitHub actions - a full compile of flash-attention can take hours
# The generic version won't be as optimized, but it will get us over the line
# RUN uv pip install --system flash_attn==2.8.2

# PyTorch + CUDA 12.4 wheel (smaller than NGC PyTorch image)
RUN uv pip install --system --no-cache-dir --index-url https://download.pytorch.org/whl/cu124 \
    torch \
    torchvision \
    torchaudio

# Copy gpu_burn from the builder
COPY --from=builder /workspace/gpu-burn/gpu_burn /usr/local/bin/
RUN mkdir -p /usr/local/share/man/man8/
COPY --from=builder /workspace/gpu-burn/gpu-burn.8 /usr/local/share/man/man8/
RUN mkdir -p /usr/local/lib/gpu_burn
COPY --from=builder /workspace/gpu-burn/compare.ptx /usr/local/lib/gpu_burn/

# Now add the model training app.
RUN uv pip install --system -e /app

# Non-root user & workspace for non-privileged wwork
RUN useradd -ms /bin/bash trainer && mkdir -p /workspace && chown -R trainer:trainer /workspace /opt
WORKDIR /app

COPY --chown=trainer:trainer scripts/ ./scripts/
COPY --chown=trainer:trainer training/ ./training/
COPY --chown=trainer:trainer configs/ ./configs/

ENV PATH="/opt/nvbandwidth:/opt/nccl-tests:/opt/hpcx/:/opt/hpcx/ucc/bin:/opt/hpcx/ucx/bin:/opt/hpcx/sharp/bin:/opt/hpcx/ompi/bin:/opt/hpcx/clusterkit/bin:/opt/hpcx/hcoll/bin:${PATH}"

# We need to be root to run some of the utils like gpu-burn.
USER root

RUN echo "Etc/UTC" > /etc/timezone
RUN DEBIAN_FRONTEND=noninteractive TZ=Etc/UTC apt-get -y install tzdata

RUN apt-get install -y software-properties-common apt-transport-https
RUN apt-get update -y
RUN wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-keyring_1.0-1_all.deb
RUN dpkg -i cuda-keyring_1.0-1_all.deb
RUN add-apt-repository "deb https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/ /"
RUN apt-get update -y
RUN apt-get install -y datacenter-gpu-manager

RUN wget -P /etc/apt/sources.list.d https://linux.mellanox.com/public/repo/mlnx_ofed/latest/ubuntu22.04/mellanox_mlnx_ofed.list
RUN wget -q -O - https://www.mellanox.com/downloads/ofed/RPM-GPG-KEY-Mellanox | gpg --dearmor -o /etc/apt/keyrings/mellanox.gpg
RUN cp /etc/apt/keyrings/mellanox.gpg /usr/share/keyrings/
RUN sed -i 's#^deb#deb [signed-by=/usr/share/keyrings/mellanox.gpg]#' /etc/apt/sources.list.d/mellanox_mlnx_ofed.list


# Install the Infiniband tools
# https://docs.nvidia.com/networking/display/ubuntu2204/important+packages+and+their+installation
# Repo no longer has the "-libs" packages for ibutils and opensm
# ibutils-libs \
# opensm-libs \
RUN apt-get install -y \
ibutils \
opensm \
rdma-core \
libibmad5 \
infiniband-diags \
perftest \
mstflint

# (Optional) DCGM tools inside container if you want to run dcgmi from here

# DCGM Exporter
COPY --from=builder /workspace/dcgm-exporter/cmd/dcgm-exporter/dcgm-exporter /usr/bin/dcgm-exporter
RUN chmod 755 /usr/bin/dcgm-exporter
RUN mkdir -p /etc/dcgm-exporter/
COPY --from=builder /workspace/dcgm-exporter/etc/default-counters.csv /etc/dcgm-exporter/default-counters.csv
RUN chmod 644 /etc/dcgm-exporter/default-counters.csv

# COPY --from=builder /workspace/dcgm /tmp/dcgm/
# WORKDIR /tmp/dcgm
# RUN make install
# WORKDIR /tmp/dcgm-exporter
# RUN make install
# If we're running systemd, enable DCGM
# RUN systemctl --now enable nvidia-dcgm

