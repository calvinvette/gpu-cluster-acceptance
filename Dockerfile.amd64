# Dockerfile.amd64: 
# ---- builder ----
# FROM nvcr.io/nvidia/cuda:12.4.1-devel-ubuntu22.04 AS builder
FROM docker.io/nvidia/cuda:12.4.1-devel-ubuntu22.04 AS builder
# ARGS: (TODO - Set here and parameterize below)
    # CUDA_VERSION=12.4.1
    # CUDA_MAJOR_VERSION=12
    # CUDA_MINOR_VERSION=4
    # CUDA_PATCH_VERSION=1
    # CUDA_CRAMMED_VERSION=124
    # HPCX_VERSION=2.23
    # UBUNTU_VERSION=22.04 # NVIDIA still has issues on 24.04
    # PYTHON_VERSION=3.12
    # TORCH_VERSION=2.8.0

RUN echo "FINDING NVCC..." && find / -name nvcc

RUN apt-get update && DEBIAN_FRONTEND=noninteractive apt-get install -y \
    git build-essential cmake libopenmpi-dev openmpi-bin && \
    rm -rf /var/lib/apt/lists/*

# nvbandwidth
RUN git clone --depth 1 https://github.com/NVIDIA/nvbandwidth /opt/nvbandwidth
WORKDIR /opt/nvbandwidth
# debian_install failed - doing these steps manually
# RUN . ./debian_install.sh
RUN apt-get -y update
RUN apt-get install -y software-properties-common apt-transport-https curl wget
RUN apt-get -y update
RUN add-apt-repository universe
RUN apt-get -y update
RUN apt-get install -y libboost-all-dev libboost-program-options-dev
RUN mkdir -p /opt/nvbandwidth/build
WORKDIR /opt/nvbandwidth/build
RUN cmake ..
RUN make -C /opt/nvbandwidth/build

# HPCX
WORKDIR /tmp
# RUN curl -LO https://content.mellanox.com/hpc/hpc-x/v2.23/hpcx-v2.23-gcc-inbox-ubuntu22.04-cuda12-x86_64.tbz
RUN curl -LO 'https://content.mellanox.com/hpc/hpc-x/v2.23/hpcx-v2.23-gcc-doca_ofed-ubuntu22.04-cuda12-x86_64.tbz'
# RUN wget -O 'https://content.mellanox.com/hpc/hpc-x/v2.23/hpcx-v2.23-gcc-doca_ofed-ubuntu22.04-cuda12-x86_64.tbz'
RUN tar -C /opt -xvpf hpcx-v2.23-gcc-doca_ofed-ubuntu22.04-cuda12-x86_64.tbz
RUN rm /tmp/hpcx-v2.23-gcc-doca_ofed-ubuntu22.04-cuda12-x86_64.tbz
RUN ln -s /opt/hpcx-v2.23-gcc-doca_ofed-ubuntu22.04-cuda12-x86_64 /opt/hpcx
# Setting the environment via script failed; doing manual ENV commands instead
# RUN . /opt/hpcx/hpcx-init.sh
ENV HPCX_DIR="/opt/hpcx"
ENV HPCX_UCX_DIR="/opt/hpcx/ucx"
ENV HPCX_UCC_DIR="/opt/hpcx/ucc"
ENV HPCX_SHARP_DIR="/opt/hpcx/sharp"
ENV HPCX_HCOLL_DIR="/opt/hpcx/hcoll"
ENV HPCX_MPI_DIR="/opt/hpcx/ompi"
ENV HPCX_OSHMEM_DIR="/opt/hpcx/ompi"
ENV HPCX_MPI_TESTS_DIR="/opt/hpcx/ompi/tests"
ENV HPCX_OSU_DIR="/opt/hpcx/ompi/tests/osu-micro-benchmarks"
ENV HPCX_OSU_CUDA_DIR="/opt/hpcx/ompi/tests/osu-micro-benchmarks-cuda"
ENV HPCX_CLUSTERKIT_DIR="/opt/hpcx/clusterkit"
ENV HPCX_NCCLNET_PLUGIN_DIR="/opt/hpcx/ncclnet_plugin"
ENV HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR="/opt/hpcx/nccl_rdma_sharp_plugin"
ENV OMPI_HOME="/opt/hpcx/ompi"
ENV MPI_HOME="/opt/hpcx/ompi"
ENV OSHMEM_HOME="/opt/hpcx/ompi"
ENV SHMEM_HOME="/opt/hpcx/ompi"
ENV PATH="/usr/local/cuda-12/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin:/opt/nvbandwidth:/opt/hpcx/:/opt/hpcx/ucc/bin:/opt/hpcx/ucx/bin:/opt/hpcx/sharp/bin:/opt/hpcx/ompi/bin:/opt/hpcx/clusterkit/bin:/opt/hpcx/hcoll/bin"
ENV LD_LIBRARY_PATH="/usr/local/lib:/usr/local/cuda-12/lib64:"

# nccl-tests
RUN git clone --depth 1 https://github.com/NVIDIA/nccl-tests /opt/nccl-tests
RUN make -C /opt/nccl-tests MPI=1




# ---- runtime ----
# FROM nvcr.io/nvidia/cuda:12.4.1-runtime-ubuntu22.04
FROM docker.io/nvidia/cuda:12.4.1-runtime-ubuntu22.04

LABEL org.opencontainers.image.licenses="AGPL-3.0-only"

# minimal runtime deps
RUN apt-get update && DEBIAN_FRONTEND=noninteractive apt-get install -y \
    python3 python3-pip python3-venv \
    rdma-core libibverbs1 libopenmpi3 openmpi-bin \
    pciutils iproute2 iputils-ping jq fio && \
    rm -rf /var/lib/apt/lists/*

# (Optional) DCGM tools inside container if you want to run dcgmi from here
# TODO - Fix this - Apparently we can't have nice things... 
# RUN apt-get update && apt-get install -y dcgm dcgm-exporter && rm -rf /var/lib/apt/lists/*

# Python deps (install only what you need)
COPY pyproject.toml /app/pyproject.toml
WORKDIR /app
RUN pip3 install --no-cache-dir --upgrade pip
RUN pip3 install packaging uvicorn uv

ENV PATH=${PATH}:/root/.local/bin/
RUN echo $PATH
# RUN find / -name uv

# PyTorch + CUDA 12.4 wheel (smaller than NGC PyTorch image)
RUN uv pip install --system --no-cache-dir --index-url https://download.pytorch.org/whl/cu124 \
    torch \
    torchvision \
    torchaudio
RUN uv pip install --system flash-attn==2.8.2 --no-build-isolation

RUN uv pip install --system -e /app

# Copy binaries from builder
COPY --from=builder /opt/nvbandwidth /opt/nvbandwidth
COPY --from=builder /opt/nccl-tests /opt/nccl-tests

# Non-root user & workspace
RUN useradd -ms /bin/bash trainer && mkdir -p /workspace && chown -R trainer:trainer /workspace /opt
USER trainer
WORKDIR /workspace

COPY --chown=trainer:trainer scripts/ ./scripts/
COPY --chown=trainer:trainer training/ ./training/
COPY --chown=trainer:trainer configs/ ./configs/
ENV PATH="/opt/nvbandwidth:/opt/nccl-tests:${PATH}"